{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.document_loaders import TextLoader, PyPDFLoader, CSVLoader\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import OpenAI\n",
    "from dotenv import load_dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vector_db(file_path, db_name=\"local_vector_db\"):\n",
    "    \"\"\"\n",
    "    Create a vector database from a text file\n",
    "    \"\"\"\n",
    "    # Load the document\n",
    "    loader = TextLoader(file_path)\n",
    "    print(\"Loaded document\")\n",
    "    # Determine file type and use appropriate loader\n",
    "    file_extension = pathlib.Path(file_path).suffix.lower()\n",
    "    \n",
    "    if file_extension == '.pdf':\n",
    "        loader = PyPDFLoader(file_path)\n",
    "        documents = loader.load()\n",
    "    elif file_extension == '.txt':\n",
    "        loader = TextLoader(file_path)\n",
    "        documents = loader.load()\n",
    "    elif file_extension in ['.xlsx', '.csv']:\n",
    "        # For Excel files, convert to CSV first\n",
    "        if file_extension == '.xlsx':\n",
    "            df = pd.read_excel(file_path)\n",
    "            # Create temporary CSV file\n",
    "            temp_csv = 'temp.csv'\n",
    "            df.to_csv(temp_csv, index=False)\n",
    "            file_path = temp_csv\n",
    "        \n",
    "        loader = CSVLoader(\n",
    "            file_path,\n",
    "            csv_args={\n",
    "                'delimiter': ',',\n",
    "                'quotechar': '\"',\n",
    "                'fieldnames': None\n",
    "            }\n",
    "        )\n",
    "        documents = loader.load()\n",
    "        print(\"Loader.loaded\")\n",
    "        # Clean up temporary CSV if created\n",
    "        if file_extension == '.xlsx' and os.path.exists('temp.csv'):\n",
    "            os.remove('temp.csv')\n",
    "            print(\"removed temp csv. \")\n",
    "    \n",
    "    documents = loader.load()\n",
    "    \n",
    "    # Split text into chunks\n",
    "    text_splitter = CharacterTextSplitter(\n",
    "        separator=\"\\n\",\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200,\n",
    "        length_function=len\n",
    "    )\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    print(\"chunked\")\n",
    "    \n",
    "    # Create embeddings (using HuggingFace embeddings for local processing)\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    )\n",
    "    print(\"Embedded\")\n",
    "    \n",
    "    # Create and save the vector store\n",
    "    vector_store = FAISS.from_documents(chunks, embeddings)\n",
    "    vector_store.save_local(db_name)\n",
    "    print(\"saved vector store\")\n",
    "    \n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_vector_db(db_name=\"local_vector_db\"):\n",
    "    \"\"\"\n",
    "    Load an existing vector database\n",
    "    \"\"\"\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    )\n",
    "    print(\"Loaded vectordb.\")\n",
    "    vector_store = FAISS.load_local(db_name, embeddings)\n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_document(query, vector_store):\n",
    "    \"\"\"\n",
    "    Query the vector store and get response from LLM\n",
    "    \"\"\"\n",
    "    # Load environment variables\n",
    "    load_dotenv()\n",
    "    openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "    # Initialize OpenAI LLM\n",
    "    print(\"sending to llm\")\n",
    "    llm = OpenAI(temperature=0, api_key=openai_api_key)\n",
    "    \n",
    "    # Create a retrieval chain\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=vector_store.as_retriever(search_kwargs={\"k\": 3})\n",
    "    )\n",
    "    \n",
    "    print(\"receiving from llm\")\n",
    "    # Get response\n",
    "    response = qa_chain.invoke(query)\n",
    "\n",
    "    # response = qa_chain.run(query)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'y:\\\\WorkLab\\\\Projects\\\\chat-llm-db'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Supported file formats: .pdf, .txt, .xlsx, .csv\n",
      "Loading vector db\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kante\\AppData\\Local\\Temp\\ipykernel_13880\\2428364933.py:5: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(\n",
      "y:\\WorkLab\\Projects\\chat-llm-doc\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded vectordb.\n",
      "fCreating new vector database... {e}\n",
      "Loaded document\n",
      "chunked\n",
      "Embedded\n",
      "saved vector store\n",
      "Vector database created successfully!\n",
      "sending to llm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kante\\AppData\\Local\\Temp\\ipykernel_13880\\1024677988.py:10: LangChainDeprecationWarning: The class `OpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAI``.\n",
      "  llm = OpenAI(temperature=0, api_key=openai_api_key)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "receiving from llm\n",
      "\n",
      "Query> what is this document about?\n",
      "\n",
      "\t Response>   This document is about a project to develop a conversational AI assistant that can answer user questions based on a provided knowledge base.\n",
      "sending to llm\n",
      "receiving from llm\n",
      "\n",
      "Query> tell me about it\n",
      "\n",
      "\t Response>   The project is focused on creating a working prototype using pre-trained LLMs like GPT-4o or LLaMA. It will have a document upload and management feature, as well as a similarity-based search function to retrieve relevant content. The AI will provide a basic web chat interface for simple queries and will display a fallback message when the knowledge base lacks information. The project will be coded in Python with Flask as the web framework and SQLite for data storage. It will only support text inputs and English as the language for responses.\n",
      "sending to llm\n",
      "receiving from llm\n",
      "\n",
      "Query> what are the requirements for it?\n",
      "\n",
      "\t Response>   The requirements for the system include performance, reliability, usability, security, and portability. These requirements specify criteria that the system should meet beyond basic functionality, such as response time, scalability, error handling, user interface simplicity, knowledge base access, and access control. The system also has limitations, such as not supporting very large datasets and only responding to text inputs in English.\n",
      "sending to llm\n",
      "receiving from llm\n",
      "\n",
      "Query> okay, whats the way to get it set up initially?\n",
      "\n",
      "\t Response>   I don't know, as this context does not provide enough information about the system being discussed.\n",
      "sending to llm\n",
      "receiving from llm\n",
      "\n",
      "Query> can you tell me about it from yourself?\n",
      "\n",
      "\t Response>   I am an AI and I do not have personal experiences or opinions. I am designed to assist with providing information and generating responses based on the knowledge base provided to me. Is there something specific you would like to know about the project?\n",
      "sending to llm\n",
      "receiving from llm\n",
      "\n",
      "Query> \n",
      "\n",
      "\t Response>   I don't know.\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    print(\"Supported file formats: .pdf, .txt, .xlsx, .csv\")\n",
    "    file_path = input(\"Enter the path to your document: \")\n",
    "\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"Error: File '{file_path}' not found!\")\n",
    "        return\n",
    "    try:\n",
    "        print(\"Loading vector db\")\n",
    "        vector_store = load_vector_db()\n",
    "        print(\"Loaded existing vector database\")\n",
    "    except Exception as e:\n",
    "        print(\"fCreating new vector database... {e}\")\n",
    "        vector_store = create_vector_db(file_path)\n",
    "        print(\"Vector database created successfully!\")\n",
    "\n",
    "    while True:\n",
    "        query = input(\"\\nEnter your question (or 'quit' to exit): \")\n",
    "        if query.lower() == 'quit':\n",
    "            break\n",
    "\n",
    "        response = query_document(query, vector_store)\n",
    "        print(\"\\nQuery>\", response['query'])\n",
    "        print(\"\\n\\t Response> \", response['result'])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
